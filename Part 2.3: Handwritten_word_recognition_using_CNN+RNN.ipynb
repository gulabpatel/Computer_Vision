{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 2.3: Handwritten_word_recognition_using_CNN+RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIDtJf736QF8"
      },
      "source": [
        "To install a package, if it is not present use following command\n",
        "\n",
        "!pip install packageName"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OItDyQqm6sEA"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9wJLT9y5lww"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "import _pickle as cPickle\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKUhtQBJ60wN"
      },
      "source": [
        "The dataset can be downloaded from \n",
        "https://drive.google.com/drive/folders/1ZiGxk6ZN5IBjtNAI4JkJK9YFehS8KNmT?usp=sharing\n",
        "\n",
        "create a directory and change the current directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biX2UZYX0ofA",
        "outputId": "45fc4fd9-a28f-482f-8805-715e1015b27e"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr4SxHFt0tBT",
        "outputId": "631e7a0a-43ce-4008-b6b0-dfc79baf0844"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90CIfCL-d-0"
      },
      "source": [
        "gdrive='/content/drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d41jRMAWNHe0"
      },
      "source": [
        "Change the myloc path according to your data folder\n",
        "\n",
        "Let create Training and Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_v8i6npDpBa"
      },
      "source": [
        "#myloc='MyDrive/shared data/17 Dec 2020/task02/word_traintestset'\n",
        "myloc='MyDrive/IITR_CV/word_traintestset'\n",
        "rootDir=gdrive+myloc\n",
        "\n",
        "\n",
        "trainDir=rootDir+'/Train'\n",
        "trainList=[]\n",
        "count=0\n",
        "for path, subdirs, files in os.walk(trainDir):\n",
        "  for name in files:\n",
        "    if not name.endswith('.png'):\n",
        "      continue  \n",
        "    count = count +1\n",
        "    imPath=os.path.join(path, name); \n",
        "    image=cv2.imread(imPath,0) ### we want to read image as gray\n",
        "    h,w=image.shape[:2]\n",
        "    image=cv2.resize(image,(int(w*32/h),32))\n",
        "    #print('Train ',count,image.shape)\n",
        "    parts=name.split('_');   \n",
        "    trainList.append([image, parts[0]])\n",
        "        \n",
        "      \n",
        "testDir=rootDir+'/Test'\n",
        "testList=[]\n",
        "count=0\n",
        "for path, subdirs, files in os.walk(testDir):\n",
        "  for name in files:\n",
        "    if not name.endswith('.png'):\n",
        "      continue         \n",
        "    count = count +1  \n",
        "    imPath=os.path.join(path, name); \n",
        "    image=cv2.imread(imPath,0) ### we want to read image as gray\n",
        "    h,w=image.shape[:2]\n",
        "    image=cv2.resize(image,(int(w*32/h),32))\n",
        "    #print('Test',count,image.shape)\n",
        "    parts=name.split('_');   \n",
        "    testList.append([image, parts[0]])     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNalIaYZJFi"
      },
      "source": [
        "#Calculate some statistical information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZbjeebxwF35D"
      },
      "source": [
        "list('word')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9nGzb0sQGRB"
      },
      "source": [
        "maxWidth=0\n",
        "maxLen=0\n",
        "CharList=[]\n",
        "for im,word in trainList:\n",
        "  h,w=im.shape[:2]\n",
        "  if maxLen<len(word):\n",
        "    maxLen=len(word)\n",
        "  if maxWidth<w:\n",
        "    maxWidth=w\n",
        "  CharList = list(set(CharList + list(word)))\n",
        "\n",
        "\n",
        "for im,word in testList:\n",
        "  h,w=im.shape[:2]\n",
        "  if maxLen<len(word):\n",
        "    maxLen=len(word)\n",
        "  if maxWidth<w:\n",
        "    maxWidth=w\n",
        "  CharList = list(set(CharList + list(word)))\n",
        "\n",
        "with open(rootDir+'/DataSet.pkl', 'wb') as f:\n",
        "  cPickle.dump([trainList,testList,CharList,maxWidth,maxLen], f, pickle.HIGHEST_PROTOCOL)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4NLrmL2ZCx_"
      },
      "source": [
        "Load Data from saved pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM_8co5XbZod",
        "outputId": "c17a8883-9835-4844-e669-23d862e32095"
      },
      "source": [
        "with open(rootDir+'/DataSet.pkl', 'rb') as f:\n",
        "  trainList,testList,CharList,maxWidth,maxLen = cPickle.load(f)  \n",
        "\n",
        "\n",
        "class create_dataset(object):\n",
        "    def __init__(self,CharList,maxWidth,maxLen,batch_size):\n",
        "      self.CharList=CharList\n",
        "      self.maxWidth=maxWidth\n",
        "      self.maxLen=int(maxLen)\n",
        "      self.numChar=int(len(CharList))\n",
        "      self.batch_size=batch_size\n",
        "    def gen(self,dataList, phase='Train'):\n",
        "        inps=[]\n",
        "        values=[]\n",
        "        labels=[]\n",
        "        label_length=[]\n",
        "        image_width=[]\n",
        "        try:\n",
        "          while 1:\n",
        "              shuffle(dataList)\n",
        "              for img, word in dataList:\n",
        "                  # print(imPath)\n",
        "                  tmp=np.zeros((self.maxLen),dtype='int32')\n",
        "                  for cc,ch in enumerate(word):\n",
        "                    chi=self.CharList.index(ch)\n",
        "                    tmp[cc]=int(chi)\n",
        "                    values.append(chi)\n",
        "                  labels.append(tmp.copy())\n",
        "\n",
        "                  label_length.append(len(word))\n",
        "                  \n",
        "                  tmpimage=np.zeros((32,self.maxWidth),dtype='uint8')             \n",
        "                  h,w=img.shape[:2]\n",
        "                  tmpimage[:h,:w]=img\n",
        "                  image_width.append( ( (w-4)//2 - 4)//2 - 2)\n",
        "\n",
        "                  inps.append(np.expand_dims(tmpimage,axis=-1))\n",
        "                  if len(labels)==self.batch_size:            \n",
        "                      yield np.asarray(inps,dtype='float32'), np.asarray(values,dtype='int32'), np.asarray(labels,dtype='int32'), np.asarray(label_length,dtype='int32'), np.asarray(image_width,dtype='int32')\n",
        "                      inps=[]\n",
        "                      values=[]\n",
        "                      labels=[]\n",
        "                      label_length=[]\n",
        "                      image_width=[]\n",
        "              if phase=='Test':\n",
        "                  break\n",
        "        except GeneratorExit:\n",
        "          print(\"Generated Finished\")\n",
        "\n",
        "batch_size=32\n",
        "# print(len(trainList))\n",
        "dataset=create_dataset(CharList,maxWidth,maxLen,batch_size)\n",
        "traindata=dataset.gen(trainList)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "RCtkc_DGQtkZ",
        "outputId": "26323d13-ee2f-499f-91c6-9f12cdf8f7ec"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "validdata=dataset.gen(testList,phase='Test')\n",
        "vimages, values,vclasses, label_length,image_width = next( validdata)\n",
        "\n",
        "# Fill out the subplots with the random images that you defined \n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(vimages[i][:,:,0])\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQUaI2slQyz3"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_iteration = 100\n",
        "display_step = 1\n",
        "\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "n_input = [28,28,1] # MNIST data input (img shape: 28*28)\n",
        "n_classes = dataset.numChar + 1 # MNIST total classes (0-9 digits)\n",
        "\n",
        "RNNnumKernelList=[128,64]\n",
        "scInitSize=np.sum(RNNnumKernelList)\n",
        "\n",
        "initializer=tf.keras.initializers.HeNormal()  \n",
        "# initializer=tf.keras.initializers.HeUniform()\n",
        "# initializer = tf.initializers.orthogonal(gain=1.0) \n",
        "# initializer = tf.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal')  \n",
        "# initializer = tf.initializers.glorot_uniform()  \n",
        "# initializer = tf.initializers.glorot_normal()  \n",
        "# initializer = tf.initializers.RandomUniform(minval=-1.0, maxval=1.0)  \n",
        "# initializer = tf.initializers.RandomNormal(mean=0, stddev=1.0)  \n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'cnn1': tf.Variable(initializer([3,3,n_input[2],32]),trainable=True),\n",
        "    'cnn2': tf.Variable(initializer([3,3,32,32]),trainable=True),\n",
        "    \n",
        "    \n",
        "    'cnn3': tf.Variable(initializer([3,3,32,64]),trainable=True),\n",
        "    'cnn4': tf.Variable(initializer([3,3,64,64]),trainable=True),    \n",
        "    \n",
        "    'cnn5': tf.Variable(initializer([3,3,64,64]),trainable=True),\n",
        "\n",
        "    'WString1':tf.Variable(initializer([4,RNNnumKernelList[0],RNNnumKernelList[0]]), trainable=True),\n",
        "    'WString2':tf.Variable(initializer([4,RNNnumKernelList[1],RNNnumKernelList[1]]), trainable=True),\n",
        "\n",
        "    'UString1':tf.Variable(initializer([4,64,RNNnumKernelList[0]]), trainable=True),\n",
        "    'UString2':tf.Variable(initializer([4,RNNnumKernelList[0],RNNnumKernelList[1]]), trainable=True),\n",
        "                       \n",
        "    'h1': tf.Variable(initializer([64*2, n_hidden_1]),trainable=True),\n",
        "    'h2': tf.Variable(initializer([n_hidden_1, n_hidden_2]),trainable=True),\n",
        "    'out': tf.Variable(initializer([n_hidden_2, n_classes]),trainable=True)\n",
        "}\n",
        "biases = {\n",
        "    'cb1': tf.Variable(tf.zeros([32]),trainable=True),\n",
        "    'cb2': tf.Variable(tf.zeros([32]),trainable=True),\n",
        "    'cb3': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "    'cb4': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "    'cb5': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "\n",
        "    'BString1': tf.Variable(tf.zeros([4,RNNnumKernelList[0]]),trainable=True),\n",
        "    'BString2': tf.Variable(tf.zeros([4,RNNnumKernelList[1]]),trainable=True),\n",
        "    \n",
        "    'b1': tf.Variable(tf.zeros([n_hidden_1]),trainable=True),\n",
        "    'b2': tf.Variable(tf.zeros([n_hidden_2]),trainable=True),\n",
        "    'out': tf.Variable(tf.zeros([n_classes]),trainable=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR7nYmkfXZ0G"
      },
      "source": [
        "\n",
        "varList=[]\n",
        "for key in weights:\n",
        "  varList.append(weights[key])\n",
        "for key in biases:\n",
        "  varList.append(biases[key])\n",
        "\n",
        "\n",
        "def lstm1Cell(inp,ct_0,ht_0):\n",
        "  #  forget gate\n",
        "  fg = tf.sigmoid(tf.matmul(inp, weights['UString1'][0]) + tf.matmul(ht_0,weights['WString1'][0])+ biases['BString1'][0])\n",
        "  #  input gate\n",
        "  ig = tf.sigmoid(tf.matmul(inp, weights['UString1'][1]) + tf.matmul(ht_0,weights['WString1'][1]) + biases['BString1'][0])\n",
        "  \n",
        "  #  gate weights\n",
        "  ctg = tf.tanh(tf.matmul(inp, weights['UString1'][2]) + tf.matmul(ht_0,weights['WString1'][2]) + biases['BString1'][0])\n",
        "  \n",
        "  #  output gate\n",
        "  og = tf.sigmoid(tf.matmul(inp, weights['UString1'][3]) + tf.matmul(ht_0,weights['WString1'][3])+ biases['BString1'][0])\n",
        "  \n",
        "  ct_i = fg*ct_0 + ig*ctg\n",
        "  ht_i = og*tf.tanh(ct_i)\n",
        "  \n",
        "  return ct_i,ht_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lstm2Cell(inp,ct_0,ht_0):\n",
        "  #  forget gate\n",
        "  fg = tf.sigmoid(tf.matmul(inp, weights['UString2'][0]) + tf.matmul(ht_0,weights['WString2'][0])+ biases['BString2'][0])\n",
        "  #  input gate\n",
        "  ig = tf.sigmoid(tf.matmul(inp, weights['UString2'][1]) + tf.matmul(ht_0,weights['WString2'][1]) + biases['BString2'][0])\n",
        "  \n",
        "  #  gate weights\n",
        "  ctg = tf.tanh(tf.matmul(inp, weights['UString2'][2]) + tf.matmul(ht_0,weights['WString2'][2]) + biases['BString2'][0])\n",
        "  \n",
        "  #  output gate\n",
        "  og = tf.sigmoid(tf.matmul(inp, weights['UString2'][3]) + tf.matmul(ht_0,weights['WString2'][3])+ biases['BString2'][0])\n",
        "  \n",
        "  ct_i = fg*ct_0 + ig*ctg\n",
        "  ht_i = og*tf.tanh(ct_i)\n",
        "  \n",
        "  return ct_i,ht_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def blsmCall(data):\n",
        "    data = tf.transpose(data,[1,0,2])\n",
        "    shape=tf.shape(data,out_type=tf.dtypes.int32)\n",
        "    # init_state_lstmString=tf.zeros((2,shape[1],self.scInitSize))\n",
        "    \n",
        "    # step - LSTM\n",
        "    def stepString(prev, x):\n",
        "      # gather previous internal state and output state\n",
        "     \n",
        "      # iterate through layers\n",
        "      ht, ct = [], []\n",
        "      inp = x\n",
        "      st=0\n",
        "      \n",
        "      ############### LSTM 1  ##################\n",
        "      ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[0]]),axis=0)\n",
        "      ct_i,ht_i=lstm1Cell(inp,ct_0,ht_0)   \n",
        "      inp = ht_i\n",
        "      ht.append(ht_i)\n",
        "      ct.append(ct_i)\n",
        "      st = st + RNNnumKernelList[0]\n",
        "\n",
        "\n",
        "      ############### LSTM 2  ##################\n",
        "      ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[1]]),axis=0)\n",
        "      ct_i,ht_i=lstm2Cell(inp,ct_0,ht_0)  \n",
        "      inp = ht_i\n",
        "      ht.append(ht_i)\n",
        "      ct.append(ct_i)\n",
        "      st = st + RNNnumKernelList[1]\n",
        "\n",
        "\n",
        "      return tf.stack([tf.concat(ct,axis=-1), tf.concat(ht,axis=-1)],axis=0)\n",
        "    \n",
        "    # tf.print('data ',tf.shape(data))\n",
        "    # tf.print('upflag ',tf.shape(upflag))\n",
        "    statesStringF = tf.scan(stepString, data, initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringF')\n",
        "    statesStringFct,statesStringFht=tf.unstack(statesStringF,axis=1)\n",
        "    statesStringFht=tf.split(statesStringFht,RNNnumKernelList,axis=-1)[-1]\n",
        "    \n",
        "    \n",
        "    statesStringB = tf.scan(stepString, tf.reverse(data,[0]), initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringB')\n",
        "    statesStringBct,statesStringBht=tf.unstack(statesStringB,axis=1)\n",
        "    statesStringBht=tf.split(statesStringBht,RNNnumKernelList,axis=-1)[-1]\n",
        "    statesStringBht= tf.reverse(statesStringBht,[0])\n",
        "    \n",
        "    rnnOut= tf.concat([statesStringFht,statesStringBht],axis=-1)\n",
        "    rnnOut = tf.transpose(rnnOut,[1,0,2])\n",
        "\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(rnnOut, weights['h1']), biases['b1'])\n",
        "    layer_1=tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
        "    #drop1=tf.nn.dropout(layer_1,rate=0.25)\n",
        "    \n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2=tf.nn.leaky_relu(layer_2, alpha=0.2)\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    # out_layer=tf.nn.sigmoid(out_layer)\n",
        "    y=tf.nn.softmax(out_layer,axis=-1)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayerNN(x):\n",
        "\n",
        "    cnn1_out=tf.add( tf.nn.conv2d( x, weights['cnn1'], [1,1,1,1], padding='VALID'), biases['cb1'])\n",
        "    cnn1_actv=tf.nn.leaky_relu(cnn1_out, alpha=0.2)\n",
        "    \n",
        "    cnn2_out=tf.add( tf.nn.conv2d( cnn1_actv, weights['cnn2'], [1,1,1,1], padding='VALID'), biases['cb2'])\n",
        "    cnn2_actv=tf.nn.leaky_relu(cnn2_out, alpha=0.2)\n",
        "    \n",
        "    pool1=tf.nn.max_pool(cnn2_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
        "    \n",
        "    cnn3_out=tf.add( tf.nn.conv2d( pool1, weights['cnn3'], [1,1,1,1], padding='VALID'), biases['cb3'])\n",
        "    cnn3_actv=tf.nn.leaky_relu(cnn3_out, alpha=0.2)\n",
        "    cnn4_out=tf.add( tf.nn.conv2d( cnn3_actv, weights['cnn4'], [1,1,1,1], padding='VALID'), biases['cb4'])\n",
        "    cnn4_actv=tf.nn.leaky_relu(cnn4_out, alpha=0.2)\n",
        "    \n",
        "    pool2=tf.nn.max_pool(cnn4_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
        "    \n",
        "    cnn5_out=tf.add( tf.nn.conv2d( pool2, weights['cnn5'], [1,1,1,1], padding='VALID'), biases['cb5'])\n",
        "    cnn5_actv=tf.nn.leaky_relu(cnn5_out, alpha=0.2)\n",
        "    \n",
        "    rnnInp=tf.reduce_max(cnn5_actv,axis=1)\n",
        "\n",
        "\n",
        "    return rnnInp\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=10.0**-7)\n",
        "# optimizer = tf.keras.optimizers.SGD( learning_rate=learning_rate, momentum=0.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIWvtjtNXsE3",
        "outputId": "0a3edba6-6e53-4215-8793-3fdf54098a79"
      },
      "source": [
        "\n",
        "iteration=0\n",
        "while iteration<training_iteration:\n",
        "  iteration = iteration + 1 \n",
        "  images, values, classes, label_length, image_width = next(traindata)\n",
        "       \n",
        "  images=tf.convert_to_tensor(images)\n",
        "  images=(images-127.5)/127.5\n",
        "  classes=tf.convert_to_tensor(classes)\n",
        "  label_length=tf.convert_to_tensor(label_length)\n",
        "  image_width=tf.convert_to_tensor(image_width)\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(varList)\n",
        "    rnnInp = multilayerNN(images)\n",
        "\n",
        "    y = blsmCall(rnnInp)\n",
        "\n",
        "    # data = tf.transpose(rnnInp,[1,0,2])\n",
        "    # shape=tf.shape(data,out_type=tf.dtypes.int32)\n",
        "    # # init_state_lstmString=tf.zeros((2,shape[1],self.scInitSize))\n",
        "    \n",
        "    # # step - LSTM\n",
        "    # def stepString(prev, x):\n",
        "    #   # gather previous internal state and output state\n",
        "     \n",
        "    #   # iterate through layers\n",
        "    #   ht, ct = [], []\n",
        "    #   inp = x\n",
        "    #   st=0\n",
        "      \n",
        "    #   ############### LSTM 1  ##################\n",
        "    #   ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[0]]),axis=0)\n",
        "    #   ct_i,ht_i=lstm1Cell(inp,ct_0,ht_0)   \n",
        "    #   inp = ht_i\n",
        "    #   ht.append(ht_i)\n",
        "    #   ct.append(ct_i)\n",
        "    #   st = st + RNNnumKernelList[0]\n",
        "\n",
        "\n",
        "    #   ############### LSTM 2  ##################\n",
        "    #   ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[1]]),axis=0)\n",
        "    #   ct_i,ht_i=lstm2Cell(inp,ct_0,ht_0)  \n",
        "    #   inp = ht_i\n",
        "    #   ht.append(ht_i)\n",
        "    #   ct.append(ct_i)\n",
        "    #   st = st + RNNnumKernelList[1]\n",
        "\n",
        "\n",
        "    #   return tf.stack([tf.concat(ct,axis=-1), tf.concat(ht,axis=-1)],axis=0)\n",
        "    \n",
        "    # # tf.print('data ',tf.shape(data))\n",
        "    # # tf.print('upflag ',tf.shape(upflag))\n",
        "    # statesStringF = tf.scan(stepString, data, initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringF')\n",
        "    # statesStringFct,statesStringFht=tf.unstack(statesStringF,axis=1)\n",
        "    # statesStringFht=tf.split(statesStringFht,RNNnumKernelList,axis=-1)[-1]\n",
        "    \n",
        "    \n",
        "    # statesStringB = tf.scan(stepString, tf.reverse(data,[0]), initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringB')\n",
        "    # statesStringBct,statesStringBht=tf.unstack(statesStringB,axis=1)\n",
        "    # statesStringBht=tf.split(statesStringBht,RNNnumKernelList,axis=-1)[-1]\n",
        "    # statesStringBht= tf.reverse(statesStringBht,[0])\n",
        "    \n",
        "    # rnnOut= tf.concat([statesStringFht,statesStringBht],axis=-1)\n",
        "    # rnnOut = tf.transpose(rnnOut,[1,0,2])\n",
        "    # )\n",
        "    # # Hidden fully connected layer with 256 neurons\n",
        "    # layer_1 = tf.add(tf.matmul(rnnOut, weights['h1']), biases['b1'])\n",
        "    # layer_1=tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
        "    # #drop1=tf.nn.dropout(layer_1,rate=0.25)\n",
        "    \n",
        "    # # Hidden fully connected layer with 256 neurons\n",
        "    # layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    # layer_2=tf.nn.leaky_relu(layer_2, alpha=0.2)\n",
        "    # # Output fully connected layer with a neuron for each class\n",
        "    # out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    # # out_layer=tf.nn.sigmoid(out_layer)\n",
        "    # y=tf.nn.softmax(out_layer,axis=-1)\n",
        "\n",
        "\n",
        "    shape=tf.shape(y,out_type=tf.dtypes.int32)\n",
        "    loss = tf.reduce_mean(tf.nn.ctc_loss(classes, y, label_length, image_width, logits_time_major=False))\n",
        "    \n",
        "  gradients = tape.gradient(loss, varList)\n",
        "  optimizer.apply_gradients(zip(gradients, varList))\n",
        "\n",
        "  \n",
        "  # Display loss per epoch step\n",
        "  if iteration % display_step == 0:\n",
        "      print(\"iteration:\", '%04d' % (iteration), \"cost={:.9f}\".format(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 0001 cost=128.785888672\n",
            "iteration: 0002 cost=150.656478882\n",
            "iteration: 0003 cost=170.224594116\n",
            "iteration: 0004 cost=168.920166016\n",
            "iteration: 0005 cost=131.985626221\n",
            "iteration: 0006 cost=117.947151184\n",
            "iteration: 0007 cost=102.914627075\n",
            "iteration: 0008 cost=99.768646240\n",
            "iteration: 0009 cost=173.971221924\n",
            "iteration: 0010 cost=213.561935425\n",
            "iteration: 0011 cost=106.121948242\n",
            "iteration: 0012 cost=120.290016174\n",
            "iteration: 0013 cost=112.534729004\n",
            "iteration: 0014 cost=168.900314331\n",
            "iteration: 0015 cost=102.981323242\n",
            "iteration: 0016 cost=97.161239624\n",
            "iteration: 0017 cost=73.612274170\n",
            "iteration: 0018 cost=154.139236450\n",
            "iteration: 0019 cost=155.807998657\n",
            "iteration: 0020 cost=157.592803955\n",
            "iteration: 0021 cost=88.730598450\n",
            "iteration: 0022 cost=118.317886353\n",
            "iteration: 0023 cost=92.044235229\n",
            "iteration: 0024 cost=92.671432495\n",
            "iteration: 0025 cost=100.219795227\n",
            "iteration: 0026 cost=110.336051941\n",
            "iteration: 0027 cost=192.044799805\n",
            "iteration: 0028 cost=107.459671021\n",
            "iteration: 0029 cost=218.776794434\n",
            "iteration: 0030 cost=111.064056396\n",
            "iteration: 0031 cost=228.915283203\n",
            "iteration: 0032 cost=162.305648804\n",
            "iteration: 0033 cost=148.162109375\n",
            "iteration: 0034 cost=186.689636230\n",
            "iteration: 0035 cost=254.852294922\n",
            "iteration: 0036 cost=143.963211060\n",
            "iteration: 0037 cost=175.585296631\n",
            "iteration: 0038 cost=125.729736328\n",
            "iteration: 0039 cost=95.576828003\n",
            "iteration: 0040 cost=86.442848206\n",
            "iteration: 0041 cost=106.296630859\n",
            "iteration: 0042 cost=174.899810791\n",
            "iteration: 0043 cost=152.669921875\n",
            "iteration: 0044 cost=148.980697632\n",
            "iteration: 0045 cost=233.633453369\n",
            "iteration: 0046 cost=155.514602661\n",
            "iteration: 0047 cost=118.626365662\n",
            "iteration: 0048 cost=115.745605469\n",
            "iteration: 0049 cost=173.281402588\n",
            "iteration: 0050 cost=214.974533081\n",
            "iteration: 0051 cost=140.376739502\n",
            "iteration: 0052 cost=67.725151062\n",
            "iteration: 0053 cost=131.880798340\n",
            "iteration: 0054 cost=109.799247742\n",
            "iteration: 0055 cost=108.883804321\n",
            "iteration: 0056 cost=123.427276611\n",
            "iteration: 0057 cost=100.812301636\n",
            "iteration: 0058 cost=63.176177979\n",
            "iteration: 0059 cost=152.773010254\n",
            "iteration: 0060 cost=154.159637451\n",
            "iteration: 0061 cost=185.658142090\n",
            "iteration: 0062 cost=36.147701263\n",
            "iteration: 0063 cost=111.168563843\n",
            "iteration: 0064 cost=100.910346985\n",
            "iteration: 0065 cost=190.447692871\n",
            "iteration: 0066 cost=88.141822815\n",
            "iteration: 0067 cost=171.685455322\n",
            "iteration: 0068 cost=62.939445496\n",
            "iteration: 0069 cost=144.537307739\n",
            "iteration: 0070 cost=129.177780151\n",
            "iteration: 0071 cost=126.241073608\n",
            "iteration: 0072 cost=173.160919189\n",
            "iteration: 0073 cost=152.530700684\n",
            "iteration: 0074 cost=133.172851562\n",
            "iteration: 0075 cost=78.337547302\n",
            "iteration: 0076 cost=134.607818604\n",
            "iteration: 0077 cost=107.147392273\n",
            "iteration: 0078 cost=145.127075195\n",
            "iteration: 0079 cost=127.226417542\n",
            "iteration: 0080 cost=146.280029297\n",
            "iteration: 0081 cost=114.688575745\n",
            "iteration: 0082 cost=209.837509155\n",
            "iteration: 0083 cost=107.584632874\n",
            "iteration: 0084 cost=94.737518311\n",
            "iteration: 0085 cost=120.908615112\n",
            "iteration: 0086 cost=145.684890747\n",
            "iteration: 0087 cost=91.276504517\n",
            "iteration: 0088 cost=138.476638794\n",
            "iteration: 0089 cost=136.738739014\n",
            "iteration: 0090 cost=147.679824829\n",
            "iteration: 0091 cost=177.211517334\n",
            "iteration: 0092 cost=161.679382324\n",
            "iteration: 0093 cost=169.470092773\n",
            "iteration: 0094 cost=104.529190063\n",
            "iteration: 0095 cost=68.920196533\n",
            "iteration: 0096 cost=175.424682617\n",
            "iteration: 0097 cost=117.838630676\n",
            "iteration: 0098 cost=159.806716919\n",
            "iteration: 0099 cost=147.636825562\n",
            "iteration: 0100 cost=150.601043701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ew3V-yAIHT5"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ6BeujKIPl-"
      },
      "source": [
        "\n",
        "with open(rootDir+'/Trainedweights.pkl', 'wb') as f:\n",
        "  cPickle.dump([weights,biases], f, pickle.HIGHEST_PROTOCOL)  \n",
        "\n",
        "print(len(testList))\n",
        "testdata=dataset.gen(testList,phase='Test')\n",
        "\n",
        "totalN=0\n",
        "totalP=0\n",
        "while True:\n",
        "  try:\n",
        "    images, values, classes, label_length, image_width = next(testdata)\n",
        "  except:\n",
        "    break\n",
        "  # print(classes)\n",
        "  images=tf.convert_to_tensor(images)\n",
        "  images=(images-127.5)/127.5\n",
        "  classes=tf.concat(tf.convert_to_tensor(classes),axis=0)\n",
        "  rnnInp = multilayerNN(images)\n",
        "  y = blsmCall(rnnInp)\n",
        "  y=tf.transpose(y,[1,0,2])\n",
        "  yencoded,log_probability=tf.nn.ctc_beam_search_decoder(y, label_length, beam_width=100, top_paths=1)\n",
        "\n",
        "  # print(len(yencoded))\n",
        "  # print(log_probability)\n",
        "  indices=yencoded[0].indices\n",
        "  yvalues=tf.cast(yencoded[0].values,dtype='int32')\n",
        "  dense_shape=yencoded[0].dense_shape\n",
        "\n",
        "  indices0,indices1=tf.unstack(indices,axis=-1)\n",
        "  # print('indices ',indices)\n",
        "  print('indices0 ',indices0)\n",
        "  # print('indices1 ',indices1)\n",
        "  print('values ',values) \n",
        "  print('yvalues ',yvalues)\n",
        "  # print('dense_shape ',dense_shape)\n",
        "  \n",
        "  parts=tf.dynamic_partition( yvalues, tf.cast(indices0,dtype='int32'), 32)\n",
        "  print(parts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}